{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2caa1f00",
   "metadata": {
    "id": "2caa1f00"
   },
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36751954",
   "metadata": {
    "id": "36751954"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zQCuH_8Qx_FK",
   "metadata": {
    "id": "zQCuH_8Qx_FK"
   },
   "source": [
    "# Connection to drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e541e17",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1727684224260,
     "user": {
      "displayName": "DAMIANA IOVARO",
      "userId": "12500258692918933283"
     },
     "user_tz": -120
    },
    "id": "8e541e17",
    "outputId": "ee9a8e89-2a07-4d2c-a784-9437f929191d"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    IN_COLAB = True\n",
    "    print(\"Running on Google Colab. \")\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"Not running on Google Colab. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "YqDnszP3yBxG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24352,
     "status": "ok",
     "timestamp": 1727684248607,
     "user": {
      "displayName": "DAMIANA IOVARO",
      "userId": "12500258692918933283"
     },
     "user_tz": -120
    },
    "id": "YqDnszP3yBxG",
    "outputId": "d6dadbf6-66a4-4a8c-f156-1e4675382495"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "vKxzLLMyXlXw",
   "metadata": {
    "id": "vKxzLLMyXlXw"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    os.chdir('/content/gdrive/MyDrive/Tesi/dataset')\n",
    "else:\n",
    "    os.chdir('./dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f55febe",
   "metadata": {
    "id": "0f55febe"
   },
   "source": [
    "# Downloading, Reading and Merging Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf6ace32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_directories(orig_path, dest_path):\n",
    "    # Check if the destination folder exists, otherwise create it\n",
    "    if not os.path.exists(dest_path):\n",
    "        os.makedirs(dest_path)\n",
    "    \n",
    "    # Iter over all files and folders in the source directory\n",
    "    for item in os.listdir(orig_path):\n",
    "        orig_item_path = os.path.join(orig_path, item)\n",
    "        \n",
    "        # Check if it is a folder\n",
    "        if os.path.isdir(orig_item_path):\n",
    "            # Defines the destination path for the folder\n",
    "            dest_item_path = os.path.join(dest_path, item)\n",
    "            \n",
    "            # Move the folder by renaming it\n",
    "            os.rename(orig_item_path, dest_item_path)\n",
    "            print(f\"Moved directory: {orig_item_path} -> {dest_item_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f185a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Check if some subdirectories are not empty\n",
    "path_to_check = \"./LUFlow/\"\n",
    "subdirs = [d for d in os.listdir(path_to_check) if os.path.isdir(os.path.join(path_to_check, d))]\n",
    "non_empty_subdirs = [d for d in subdirs if os.listdir(os.path.join(path_to_check, d))]\n",
    "\n",
    "dest_path = './LUFlow'\n",
    "\n",
    "if non_empty_subdirs:\n",
    "    print(\"Non-empty subdirectories:\", non_empty_subdirs)\n",
    "    print(\"Skip downloading.\")\n",
    "else:\n",
    "    print(\"All subdirectories are empty.\")\n",
    "    print(\"Download dataset.\")\n",
    "\n",
    "    # Download latest version\n",
    "    path = kagglehub.dataset_download(\"mryanm/luflow-network-intrusion-detection-data-set\")\n",
    "\n",
    "    print(\"Path to dataset files:\", path)\n",
    "\n",
    "    move_directories(path, dest_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6def2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset = './LUFlow/encoded_dataset.csv'\n",
    "\n",
    "if os.path.isfile(encoded_dataset):\n",
    "    print(\"Encoded dataset is in the directory.\")\n",
    "    ENCODED = True\n",
    "else:\n",
    "    print(\"Encoded dataset is not in the directory.\")\n",
    "    ENCODED = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dd63090",
   "metadata": {
    "id": "2dd63090"
   },
   "outputs": [],
   "source": [
    "if not ENCODED:\n",
    "\n",
    "    df_list = []\n",
    "    i = 0\n",
    "\n",
    "    selected_columns = ['bytes_in', 'bytes_out', 'num_pkts_out', 'num_pkts_in', 'proto', 'time_start', 'label']\n",
    "    dtype_dict = {'bytes_in': np.int32, 'bytes_out': np.int32, 'num_pkts_out': np.int32, \n",
    "                  'num_pkts_in': np.int32, 'proto': np.int32, 'time_start': np.int64, 'label': str}\n",
    "\n",
    "    for root, dirs, files in os.walk(dest_path):\n",
    "        for file in files:\n",
    "            # checks if 'file' does not exist in the directory\n",
    "            # checks if 'csv' is in the file name\n",
    "            # checks if a particular string is in the file name\n",
    "            if not os.path.isfile(file) and 'csv' in file:\n",
    "                chunk_iter = pd.read_csv(os.path.join(root, file), chunksize=10000, usecols=selected_columns, dtype=dtype_dict)\n",
    "                for chunk in chunk_iter:\n",
    "                    df_list.append(chunk)\n",
    "                \n",
    "                del chunk_iter\n",
    "                gc.collect()\n",
    "                i += 1\n",
    "\n",
    "    df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    del df_list\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4aaaad63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_dataset = './LUFlow/sorted_encoded_dataset.csv'\n",
    "\n",
    "# if os.path.isfile(sorted_dataset) and ENCODED:\n",
    "#     print(\"Sorted dataset is in the directory.\")\n",
    "# elif ENCODED:\n",
    "#     print(\"Sorting dataset.\")\n",
    "\n",
    "#     dataset_encoded = pd.read_csv(encoded_dataset)\n",
    "#     dataset_encoded_sorted = dataset_encoded.sort_values(by=['time_start'])\n",
    "\n",
    "#     dataset_encoded_sorted.to_csv(sorted_dataset, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f82214a",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUART = True\n",
    "\n",
    "subset = 500\n",
    "\n",
    "if ENCODED and QUART:\n",
    "    import pandas as pd\n",
    "\n",
    "    # Count number of lines in the file\n",
    "    # with open(sorted_dataset) as f:\n",
    "    with open(encoded_dataset) as f:\n",
    "        total_rows = sum(1 for _ in f) - 1  # exclude header\n",
    "\n",
    "    # Compute the fraction\n",
    "    n_quarter_rows = total_rows // subset\n",
    "\n",
    "    # Load only first quarter of the dataset\n",
    "    # df = pd.read_csv(sorted_dataset, nrows=n_quarter_rows)\n",
    "    df = pd.read_csv(encoded_dataset, nrows=n_quarter_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d202a8",
   "metadata": {
    "id": "81d202a8"
   },
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1424708b",
   "metadata": {
    "id": "1424708b"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if not ENCODED:\n",
    "    # Count the occurrences of each label\n",
    "    label_counts = df['label'].value_counts()\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.pie(label_counts, labels=label_counts.index, autopct=lambda p: '{:.0f}\\n({:.1f}%)'.format(p * sum(label_counts) / 100, p))\n",
    "\n",
    "    # Show the plot\n",
    "    plt.title('Distribution of Labels')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6118ed4",
   "metadata": {
    "id": "e6118ed4"
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a8cc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(axis=1, inplace = True)\n",
    "\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d04422fa",
   "metadata": {
    "id": "d04422fa"
   },
   "outputs": [],
   "source": [
    "if not ENCODED:\n",
    "    df = df[df['label'].isin(['benign', 'malicious'])].copy()\n",
    "\n",
    "    df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78134a11",
   "metadata": {
    "id": "78134a11"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "if not ENCODED:\n",
    "    label_encoder = LabelEncoder()\n",
    "\n",
    "    # Fit and transform the label column\n",
    "    df['label'] = label_encoder.fit_transform(df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02357a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not ENCODED:\n",
    "    df.to_csv(encoded_dataset, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k90MxbIk_G_J",
   "metadata": {
    "id": "k90MxbIk_G_J"
   },
   "source": [
    "## Construction of Interval Information Granules\n",
    "\n",
    "### Selecting index using time windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8d28361",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['time_start'].notna() & (df['time_start'] > 1e12), 'time_start'] //= 1000\n",
    "df = df.sort_values(by=['time_start'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9cf40cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "perc = 0.8\n",
    "\n",
    "rows_perc = int(len(df)*perc)\n",
    "\n",
    "df_train = df.iloc[:rows_perc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "078b4c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('./LUFlow/GranulatedData'):\n",
    "    GRANULATED = True\n",
    "else:\n",
    "    GRANULATED = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vyXIigQju030",
   "metadata": {
    "id": "vyXIigQju030"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# time_slices = [16, 128, 256, 1024, 2048, 3016]\n",
    "time_slices = [1, 8, 16, 32, 64, 128]\n",
    "\n",
    "if not GRANULATED:\n",
    "  matching_indices_slices = {}\n",
    "\n",
    "  time_start_values = df_train['time_start'].values\n",
    "  n = time_start_values\n",
    "\n",
    "  for ts in tqdm(time_slices):\n",
    "    ts_milli = ts * 1000\n",
    "    # maximum time limit of the granule\n",
    "    limit = time_start_values[0] + ts_milli\n",
    "\n",
    "    # number of groups for time_slice\n",
    "    num_group = 0\n",
    "    group_dict = {num_group:[0]}\n",
    "\n",
    "    for i in range(1, len(n)):\n",
    "        # if the value of time_start is greater than limit then limit is updated and\n",
    "        # it is initilized a new group otherwise the index of the row is added to group_dict\n",
    "        if time_start_values[i] >= limit:\n",
    "          limit = time_start_values[i] + ts_milli\n",
    "          num_group += 1\n",
    "\n",
    "          group_dict[num_group] = [i]\n",
    "        else:\n",
    "          group_dict[num_group].append(i)\n",
    "    \n",
    "    matching_indices_slices[ts] = group_dict\n",
    "    del group_dict\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zZMBBfdMA4NV",
   "metadata": {
    "id": "zZMBBfdMA4NV"
   },
   "source": [
    "### Drop Label column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96cee97",
   "metadata": {
    "id": "d96cee97"
   },
   "outputs": [],
   "source": [
    "# Separate features (X) and target variable (y)\n",
    "X_train = df_train.loc[:, df_train.columns != 'label']\n",
    "y_train = df_train['label']\n",
    "y_train = y_train.to_frame()\n",
    "\n",
    "del df, df_train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t8o0K9k6npXH",
   "metadata": {
    "id": "t8o0K9k6npXH"
   },
   "source": [
    "### Creating interval information granules with both approach to compute time spent creating granules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7fda03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "columns = ['num_frames_clust', 'time_clust', 'num_frames_just', 'time_just']\n",
    "\n",
    "if not GRANULATED:\n",
    "  # alpha = 2.0\n",
    "  # alpha = 1.0\n",
    "  # alpha = 0.5\n",
    "  # alpha = 0.1\n",
    "  # alpha = 0.01\n",
    "  alpha = 0.01\n",
    "  l = 100\n",
    "\n",
    "  a_int = int(alpha)\n",
    "  \n",
    "  for time_slice in time_slices:\n",
    "    \n",
    "    df = pd.DataFrame(columns=columns)\n",
    "    df.to_csv(f'../time_comparison_{subset}_{time_slice}.csv', index=False)\n",
    "\n",
    "    for group in tqdm(matching_indices_slices[time_slice]):\n",
    "\n",
    "      group_indeces = matching_indices_slices[time_slice][group]\n",
    "\n",
    "      group_data = X_train.iloc[group_indeces]\n",
    "\n",
    "      N = len(group_data)\n",
    "\n",
    "      if N == 1:\n",
    "        granule_data_y = y_train.loc[group_data.index]\n",
    "\n",
    "        # Time spent by Justifiable Granularity\n",
    "        start_just = time.time()\n",
    "        row = group_data.iloc[0]\n",
    "        row['label'] = granule_data_y['label'].iloc[0]\n",
    "        row = row.to_frame().T\n",
    "        end_just = time.time()\n",
    "\n",
    "        # Time spent by Clustering\n",
    "        start_clust = time.time()\n",
    "        row = group_data.iloc[0]\n",
    "        row['label'] = granule_data_y['label'].iloc[0]\n",
    "        end_clust = time.time()\n",
    "\n",
    "        time_spent_just = end_just - start_just\n",
    "        time_spent_clust = end_clust - start_clust\n",
    "\n",
    "        len_just = N\n",
    "        len_clust = N\n",
    "\n",
    "        time_row = {'num_frames_clust': len_clust, 'time_clust': time_spent_clust, 'num_frames_just': len_just, 'time_just': time_spent_just}\n",
    "        time_row = pd.DataFrame([time_row])\n",
    "        time_row.to_csv(f'../time_comparison_{subset}_{time_slice}.csv', mode='a', header=False, index=False)\n",
    "\n",
    "        continue\n",
    "      \n",
    "      stats = group_data['time_start'].agg(['min', 'max', 'mean'])\n",
    "      y_min, y_max, m = stats['min'], stats['max'], stats['mean']\n",
    "\n",
    "      y_max_rel = y_max - y_min\n",
    "\n",
    "      m_rel = m - y_min\n",
    "\n",
    "      delta_y = (y_max_rel - m_rel)/l\n",
    "\n",
    "      V_b_opt, V_a_opt = float('-inf'), float('-inf')\n",
    "      a_opt, b_opt = None, None\n",
    "\n",
    "      for h in range(0, l):\n",
    "        b_rel = m_rel+h*delta_y\n",
    "        a_rel = m_rel-h*delta_y\n",
    "\n",
    "        b = y_min + b_rel\n",
    "        sp_b = math.exp(-alpha*abs(m_rel-b_rel))\n",
    "\n",
    "        a = y_min + a_rel\n",
    "        sp_a = math.exp(-alpha*abs(a_rel-m_rel))\n",
    "\n",
    "        cov_b = ((group_data['time_start'] > m) & (group_data['time_start'] <= b)).sum() / N\n",
    "        cov_a = ((group_data['time_start'] > a) & (group_data['time_start'] <= m)).sum() / N\n",
    "\n",
    "        V_b = cov_b*sp_b\n",
    "        V_a = cov_a*sp_a\n",
    "\n",
    "        if V_a > V_a_opt:\n",
    "          a_opt, V_a_opt = a, V_a\n",
    "\n",
    "        if V_b > V_b_opt:\n",
    "          b_opt, V_b_opt = b, V_b\n",
    "\n",
    "      condition = (group_data['time_start'] > a_opt) & (group_data['time_start'] <= b_opt)\n",
    "      filtered_granule = group_data[condition]\n",
    "\n",
    "      if filtered_granule.empty:\n",
    "\n",
    "        granule_data_y = y_train.loc[group_data.index]['label']\n",
    "\n",
    "        # DA DEFINIRE COME TRATTARE IN QUESTO CASO JUSTIFIABLE GRANULARITY\n",
    "        # Time spent by Justifiable Granularity\n",
    "        start_just = time.time()\n",
    "        row = group_data.iloc[0]\n",
    "        row['label'] = granule_data_y.iloc[0]\n",
    "        # row['label'] = granule_data_y.iat[0]\n",
    "        end_just = time.time()\n",
    "        \n",
    "        # Time spent by Clustering\n",
    "        # granule_data_y = y_train.loc[group_indeces]\n",
    "        start_clust = time.time()\n",
    "        row = group_data[['bytes_in', 'bytes_out', 'num_pkts_out', 'num_pkts_in', 'proto']].sum()\n",
    "        row['time_start'] = group_data['time_start'].iloc[0]\n",
    "        row['label'] = granule_data_y.mode().iloc[0]\n",
    "        end_clust = time.time()\n",
    "\n",
    "        # Compute the time spent for each method and save to csv\n",
    "        time_spent_just = end_just - start_just\n",
    "        time_spent_clust = end_clust - start_clust\n",
    "\n",
    "        len_just = 1\n",
    "        len_clust = N\n",
    "\n",
    "        time_row = {'num_frames_clust': len_clust, 'time_clust': time_spent_clust, 'num_frames_just': len_just, 'time_just': time_spent_just}\n",
    "        time_row = pd.DataFrame([time_row])\n",
    "        time_row.to_csv(f'../time_comparison_{subset}_{time_slice}.csv', mode='a', header=False, index=False)\n",
    "\n",
    "        continue  \n",
    "\n",
    "      selected_rows = y_train.loc[filtered_granule.index]\n",
    "\n",
    "      # Time spent by Justifiable Granularity\n",
    "      start_just = time.time()\n",
    "      # if len(filtered_granule) == 1:\n",
    "      #   row = filtered_granule.iloc[0]\n",
    "      # else:\n",
    "      row = filtered_granule[['bytes_in', 'bytes_out', 'num_pkts_out', 'num_pkts_in', 'proto']].sum()      \n",
    "      row['time_start'] = filtered_granule['time_start'].iloc[0]\n",
    "      row['label'] = selected_rows.mode().iloc[0]\n",
    "      end_just = time.time()\n",
    "\n",
    "      # Time spent by Clustering\n",
    "      granule_data_y = y_train.loc[group_data.index]['label']\n",
    "      start_clust = time.time()\n",
    "      row = group_data[['bytes_in', 'bytes_out', 'num_pkts_out', 'num_pkts_in', 'proto']].sum()\n",
    "      row['time_start'] = group_data['time_start'].iloc[0]\n",
    "      row['label'] = granule_data_y.mode().iloc[0]\n",
    "      end_clust = time.time()\n",
    "\n",
    "      # Compute the time spent for each method and save to csv\n",
    "      time_spent_just = end_just - start_just\n",
    "      time_spent_clust = end_clust - start_clust\n",
    "\n",
    "      len_just = len(filtered_granule)\n",
    "      len_clust = N\n",
    "\n",
    "      time_row = {'num_frames_clust': len_clust, 'time_clust': time_spent_clust, 'num_frames_just': len_just, 'time_just': time_spent_just}\n",
    "      time_row = pd.DataFrame([time_row])\n",
    "      time_row.to_csv(f'../time_comparison_{subset}_{time_slice}.csv', mode='a', header=False, index=False)\n",
    "      # print(time_row)\n",
    "\n",
    "      # del group_data, filtered_granule, selected_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b06b1541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_int = int(alpha)\n",
    "mean_file = f'../mean_{subset}.csv'\n",
    "std_file = f'../std_{subset}.csv'\n",
    "\n",
    "columns = ['num_frames_clust', 'time_clust', 'num_frames_just', 'time_just', 'time_slice', 'perc_red']\n",
    "\n",
    "df_col = pd.DataFrame(columns=columns)\n",
    "df_col.to_csv(mean_file, index=False)\n",
    "df_col.to_csv(std_file, index=False)\n",
    "\n",
    "\n",
    "for time_slice in time_slices:\n",
    "    df = pd.read_csv(f'../time_comparison_{subset}_{time_slice}.csv')\n",
    "\n",
    "    mean = df.mean()\n",
    "    std = df.std()\n",
    "\n",
    "    mean = mean.to_frame().T\n",
    "    std = std.to_frame().T\n",
    "\n",
    "    mean['time_slice'] = time_slice\n",
    "    std['time_slice'] = time_slice\n",
    "\n",
    "    perc_red = ((df['num_frames_clust'] - df['num_frames_just']) / df['num_frames_clust']) * 100\n",
    "    perc_red_mean = perc_red.mean()\n",
    "    perc_red_std = perc_red.std()\n",
    "\n",
    "    mean['perc_red'] = perc_red_mean\n",
    "    std['perc_red'] = perc_red_std\n",
    "\n",
    "    mean.to_csv(mean_file, mode='a', header=False, index=False)\n",
    "    std.to_csv(std_file, mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4399f0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = np.arange(len(time_slices))\n",
    "figure, axis = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "mean_df = pd.read_csv(mean_file)\n",
    "std_df = pd.read_csv(std_file)\n",
    "\n",
    "axis[0, 0].plot(x_axis, mean_df['time_clust'], label='Clustering')\n",
    "axis[0, 0].plot(x_axis, mean_df['time_just'], label='Justifiable Granularity')\n",
    "axis[0, 0].set_title('Mean Time Comparison')\n",
    "axis[0, 0].set_xticks(x_axis)  \n",
    "axis[0, 0].set_xticklabels(time_slices) \n",
    "axis[0, 0].set_xlabel('Time Slice')\n",
    "\n",
    "axis[0, 1].plot(x_axis, std_df['time_clust'], label='Clustering')\n",
    "axis[0, 1].plot(x_axis, std_df['time_just'], label='Justifiable Granularity')\n",
    "axis[0, 1].set_title('Standard Deviation Time Comparison')\n",
    "axis[0, 1].set_xticks(x_axis)  \n",
    "axis[0, 1].set_xticklabels(time_slices) \n",
    "axis[0, 1].set_xlabel('Time Slice')\n",
    "\n",
    "axis[1, 0].plot(x_axis, mean_df['num_frames_clust'], label='Clustering')\n",
    "axis[1, 0].plot(x_axis, mean_df['num_frames_just'], label='Justifiable Granularity')\n",
    "axis[1, 0].set_title('Mean of frames number')\n",
    "axis[1, 0].set_xticks(x_axis)  \n",
    "axis[1, 0].set_xticklabels(time_slices) \n",
    "axis[1, 0].set_xlabel('Time Slice')\n",
    "\n",
    "axis[1, 1].plot(x_axis, std_df['num_frames_clust'], label='Clustering')\n",
    "axis[1, 1].plot(x_axis, std_df['num_frames_just'], label='Justifiable Granularity')\n",
    "axis[1, 1].set_title('Standard Deviation of frames number')\n",
    "axis[1, 1].set_xticks(x_axis)  \n",
    "axis[1, 1].set_xticklabels(time_slices) \n",
    "axis[1, 1].set_xlabel('Time Slice')\n",
    "\n",
    "plt.tight_layout() # rect=[0, 0.1, 1, 1]\n",
    "# plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "lines, labels = [], []\n",
    "for ax in axis.flat:\n",
    "    for line, label in zip(*ax.get_legend_handles_labels()):\n",
    "        if label not in labels:  # Avoid duplicate labels\n",
    "            lines.append(line)\n",
    "            labels.append(label)\n",
    "\n",
    "figure.legend(lines, labels, loc='lower center', ncol=2, fontsize='medium', bbox_to_anchor=(0.5, -0.05))\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3475ee17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "for time_slice in time_slices:\n",
    "    df = pd.read_csv(f'../time_comparison_{subset}_{time_slice}.csv')\n",
    "    X_axis = np.arange(len(df))\n",
    "    plt.plot(X_axis, df['time_clust'], label='Clustering')\n",
    "    plt.plot(X_axis, df['time_just'], label='Justifiable Granularity')\n",
    "\n",
    "    plt.xlabel('Granule')\n",
    "    plt.ylabel('Time (s)')\n",
    "    plt.title(f'Time comparison for time slice {time_slice}')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e053e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "for time_slice in time_slices:\n",
    "    df = pd.read_csv(f'../time_comparison_{subset}_{time_slice}.csv')\n",
    "    X_axis = np.arange(len(df))\n",
    "    plt.plot(X_axis, df['num_frames_clust'], label='Clustering')\n",
    "    plt.plot(X_axis, df['num_frames_just'], label='Justifiable Granularity')\n",
    "\n",
    "    plt.xlabel('Granule')\n",
    "    plt.ylabel('Number of frames')\n",
    "    plt.title(f'Number of frames comparison for time slice {time_slice}')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 975848,
     "sourceId": 7295614,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30626,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 459.535705,
   "end_time": "2024-01-02T19:22:34.876445",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-01-02T19:14:55.340740",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
