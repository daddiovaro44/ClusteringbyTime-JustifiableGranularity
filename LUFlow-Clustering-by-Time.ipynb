{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2caa1f00",
   "metadata": {
    "id": "2caa1f00"
   },
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36751954",
   "metadata": {
    "id": "36751954"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zQCuH_8Qx_FK",
   "metadata": {
    "id": "zQCuH_8Qx_FK"
   },
   "source": [
    "# Connection to drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e541e17",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1727081436320,
     "user": {
      "displayName": "DAMIANA IOVARO",
      "userId": "12500258692918933283"
     },
     "user_tz": -120
    },
    "id": "8e541e17",
    "outputId": "f8eccca4-9bb8-42b5-82f7-0bef66d2443a"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    IN_COLAB = True\n",
    "    print(\"Running on Google Colab. \")\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"Not running on Google Colab. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "YqDnszP3yBxG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 115659,
     "status": "ok",
     "timestamp": 1727081551964,
     "user": {
      "displayName": "DAMIANA IOVARO",
      "userId": "12500258692918933283"
     },
     "user_tz": -120
    },
    "id": "YqDnszP3yBxG",
    "outputId": "5180063c-979d-49fd-aecf-072a5c388b3b"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "vKxzLLMyXlXw",
   "metadata": {
    "id": "vKxzLLMyXlXw"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    os.chdir('/content/gdrive/MyDrive/Tesi/dataset')\n",
    "else:\n",
    "    os.chdir('./dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f55febe",
   "metadata": {
    "id": "0f55febe"
   },
   "source": [
    "# Downloading, Reading and Merging Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7de302cf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 482,
     "status": "ok",
     "timestamp": 1727081556848,
     "user": {
      "displayName": "DAMIANA IOVARO",
      "userId": "12500258692918933283"
     },
     "user_tz": -120
    },
    "id": "7de302cf",
    "outputId": "c65544e3-4540-4fce-c2d9-a0fa125abc7c"
   },
   "outputs": [],
   "source": [
    "def move_directories(orig_path, dest_path):\n",
    "    # Check if the destination folder exists, otherwise create it\n",
    "    if not os.path.exists(dest_path):\n",
    "        os.makedirs(dest_path)\n",
    "    \n",
    "    # Iter over all files and folders in the source directory\n",
    "    for item in os.listdir(orig_path):\n",
    "        orig_item_path = os.path.join(orig_path, item)\n",
    "        \n",
    "        # Check if it is a folder\n",
    "        if os.path.isdir(orig_item_path):\n",
    "            # Defines the destination path for the folder\n",
    "            dest_item_path = os.path.join(dest_path, item)\n",
    "            \n",
    "            # Move the folder by renaming it\n",
    "            os.rename(orig_item_path, dest_item_path)\n",
    "            print(f\"Moved directory: {orig_item_path} -> {dest_item_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6329a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Check if some subdirectories are not empty\n",
    "path_to_check = \"./LUFlow/\"\n",
    "subdirs = [d for d in os.listdir(path_to_check) if os.path.isdir(os.path.join(path_to_check, d))]\n",
    "non_empty_subdirs = [d for d in subdirs if os.listdir(os.path.join(path_to_check, d))]\n",
    "\n",
    "dest_path = './LUFlow'\n",
    "\n",
    "if non_empty_subdirs:\n",
    "    print(\"Non-empty subdirectories:\", non_empty_subdirs)\n",
    "    print(\"Skip downloading.\")\n",
    "else:\n",
    "    print(\"All subdirectories are empty.\")\n",
    "    print(\"Download dataset.\")\n",
    "\n",
    "    # Download latest version\n",
    "    path = kagglehub.dataset_download(\"mryanm/luflow-network-intrusion-detection-data-set\")\n",
    "\n",
    "    print(\"Path to dataset files:\", path)\n",
    "\n",
    "    move_directories(path, dest_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f6e241",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset = './LUFlow/encoded_dataset.csv'\n",
    "\n",
    "if os.path.isfile(encoded_dataset):\n",
    "    print(\"Encoded dataset is in the directory.\")\n",
    "    ENCODED = True\n",
    "else:\n",
    "    print(\"Encoded dataset is not in the directory.\")\n",
    "    ENCODED = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dd63090",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43347,
     "status": "ok",
     "timestamp": 1727081606898,
     "user": {
      "displayName": "DAMIANA IOVARO",
      "userId": "12500258692918933283"
     },
     "user_tz": -120
    },
    "id": "2dd63090",
    "outputId": "2accc41b-3a2f-4ae1-9488-d1dce30ba628"
   },
   "outputs": [],
   "source": [
    "if not ENCODED:\n",
    "\n",
    "    df_list = []\n",
    "    i = 0\n",
    "\n",
    "    selected_columns = ['bytes_in', 'bytes_out', 'num_pkts_out', 'num_pkts_in', 'proto', 'time_start', 'label']\n",
    "    dtype_dict = {'bytes_in': np.int32, 'bytes_out': np.int32, 'num_pkts_out': np.int32, \n",
    "                  'num_pkts_in': np.int32, 'proto': np.int32, 'time_start': np.int64, 'label': str}\n",
    "\n",
    "    for root, dirs, files in os.walk(dest_path):\n",
    "        for file in files:\n",
    "            # checks if 'file' does not exist in the directory\n",
    "            # checks if 'csv' is in the file name\n",
    "            # checks if a particular string is in the file name\n",
    "            if not os.path.isfile(file) and 'csv' in file:\n",
    "                chunk_iter = pd.read_csv(os.path.join(root, file), chunksize=10000, usecols=selected_columns, dtype=dtype_dict)\n",
    "                for chunk in chunk_iter:\n",
    "                    df_list.append(chunk)\n",
    "                \n",
    "                del chunk_iter\n",
    "                gc.collect()\n",
    "                i += 1\n",
    "\n",
    "    df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    del df_list\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba43238",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dataset = './LUFlow/sorted_encoded_dataset.csv'\n",
    "\n",
    "if os.path.isfile(sorted_dataset) and ENCODED:\n",
    "    print(\"Sorted dataset is in the directory.\")\n",
    "else:\n",
    "    print(\"Sorting dataset.\")\n",
    "\n",
    "    dataset_encoded = pd.read_csv(encoded_dataset)\n",
    "    dataset_encoded_sorted = dataset_encoded.sort_values(by=['time_start'])\n",
    "\n",
    "    dataset_encoded_sorted.to_csv(sorted_dataset, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16117c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUART = True\n",
    "\n",
    "subset = 300\n",
    "\n",
    "if ENCODED and QUART:\n",
    "    import pandas as pd\n",
    "\n",
    "    # Count number of lines in the file\n",
    "    with open(encoded_dataset) as f:\n",
    "        total_rows = sum(1 for _ in f) - 1  # exclude header\n",
    "\n",
    "    # Compute the fraction\n",
    "    n_quarter_rows = total_rows // subset\n",
    "\n",
    "    # Load only first quarter of the dataset\n",
    "    df = pd.read_csv(encoded_dataset, nrows=n_quarter_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d202a8",
   "metadata": {
    "id": "81d202a8"
   },
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1424708b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 675
    },
    "executionInfo": {
     "elapsed": 612,
     "status": "ok",
     "timestamp": 1727081610290,
     "user": {
      "displayName": "DAMIANA IOVARO",
      "userId": "12500258692918933283"
     },
     "user_tz": -120
    },
    "id": "1424708b",
    "outputId": "09dba248-d29a-42a2-e9fc-60758edbe581"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if not ENCODED:\n",
    "    # Count the occurrences of each label\n",
    "    label_counts = df['label'].value_counts()\n",
    "\n",
    "    # Plot using Matplotlib\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.pie(label_counts, labels=label_counts.index, autopct=lambda p: '{:.0f}\\n({:.1f}%)'.format(p * sum(label_counts) / 100, p))\n",
    "\n",
    "    # Show the plot\n",
    "    plt.title('Distribution of Labels')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Label counts: {label_counts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6118ed4",
   "metadata": {
    "id": "e6118ed4"
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04422fa",
   "metadata": {
    "id": "d04422fa"
   },
   "outputs": [],
   "source": [
    "df.dropna(axis=1, inplace = True)\n",
    "\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "kYo4jc63q_vs",
   "metadata": {
    "id": "kYo4jc63q_vs"
   },
   "outputs": [],
   "source": [
    "if not ENCODED:\n",
    "    df = df[df['label'].isin(['benign', 'malicious'])].copy()\n",
    "\n",
    "    df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66c37230",
   "metadata": {
    "id": "66c37230"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "if not ENCODED:\n",
    "    label_encoder = LabelEncoder()\n",
    "\n",
    "    # Fit and transform the label column\n",
    "    df['label'] = label_encoder.fit_transform(df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37f9c26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not ENCODED:\n",
    "    df.to_csv(encoded_dataset, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1241d8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 808,
     "status": "ok",
     "timestamp": 1726840102354,
     "user": {
      "displayName": "DAMIANA IOVARO",
      "userId": "12500258692918933283"
     },
     "user_tz": -120
    },
    "id": "fe1241d8",
    "outputId": "f256d0f8-b218-4f9b-f799-8deec570652e"
   },
   "outputs": [],
   "source": [
    "# Count the occurrences of each label\n",
    "label_counts = df['label'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(label_counts, labels=label_counts.index, autopct=lambda p: '{:.0f}\\n({:.1f}%)'.format(p * sum(label_counts) / 100, p))\n",
    "\n",
    "# Show the plot\n",
    "plt.title('Distribution of Labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_tt-YtWchGYq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1726840104422,
     "user": {
      "displayName": "DAMIANA IOVARO",
      "userId": "12500258692918933283"
     },
     "user_tz": -120
    },
    "id": "_tt-YtWchGYq",
    "outputId": "ac291d34-8b56-4cbc-bf4c-059c274a43fc"
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AyNpUk3wz7No",
   "metadata": {
    "id": "AyNpUk3wz7No"
   },
   "source": [
    "### Preparation of index for the granules varying time window dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ee0732d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['time_start'].notna() & (df['time_start'] > 1e12), 'time_start'] //= 1000\n",
    "df = df.sort_values(by=['time_start'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba0dd0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "perc = 0.8\n",
    "\n",
    "rows_perc = int(len(df)*perc)\n",
    "\n",
    "df_test = df.iloc[rows_perc:]\n",
    "df_train = df.iloc[:rows_perc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d66c5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('./LUFlow/GranuleData'):\n",
    "    GRANULATED = True\n",
    "else:\n",
    "    GRANULATED = False\n",
    "    os.makedirs('./LUFlow/GranuleData')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vyXIigQju030",
   "metadata": {
    "id": "vyXIigQju030"
   },
   "outputs": [],
   "source": [
    "time_slices = [16, 128, 256, 1024, 2048, 3016]\n",
    "\n",
    "if not GRANULATED:\n",
    "  matching_indices_slices = {}\n",
    "\n",
    "  time_start_values = df['time_start'].values\n",
    "  n = time_start_values\n",
    "\n",
    "  for ts in tqdm(time_slices):\n",
    "    ts_milli = ts * 1000\n",
    "    # maximum time limit of the granule\n",
    "    limit = time_start_values[0] + ts_milli\n",
    "    # number of granules\n",
    "    num_gran = 0\n",
    "    group_dict = {num_gran:[0]}\n",
    "\n",
    "    for i in range(1, len(n)):\n",
    "        # if the value of time_start is greater than limit then limit is updated and\n",
    "        # it is initilized a new granule\n",
    "        # otherwise the index of the row is added to matching_indeces_slices\n",
    "        if time_start_values[i] >= limit:\n",
    "          limit = time_start_values[i] + ts_milli\n",
    "          num_gran += 1\n",
    "\n",
    "          group_dict[num_gran] = [i]\n",
    "        else:\n",
    "          group_dict[num_gran].append(i)\n",
    "\n",
    "    matching_indices_slices[ts] = group_dict\n",
    "    del group_dict\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96cee97",
   "metadata": {
    "id": "d96cee97"
   },
   "outputs": [],
   "source": [
    "# Separate features (X) and target variable (y)\n",
    "X_train = df.loc[:, df.columns != 'label']\n",
    "y_train = df['label']\n",
    "y_train = y_train.to_frame()\n",
    "\n",
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f387336a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features (X) and target variable (y)\n",
    "X_test = df_test.loc[:, df_test.columns != 'label']\n",
    "y_test = df_test['label']\n",
    "\n",
    "\n",
    "del df_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911c3e12",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "executionInfo": {
     "elapsed": 639,
     "status": "ok",
     "timestamp": 1726841129862,
     "user": {
      "displayName": "DAMIANA IOVARO",
      "userId": "12500258692918933283"
     },
     "user_tz": -120
    },
    "id": "911c3e12",
    "outputId": "8bda956e-cc1e-4533-8d47-3c577d718165"
   },
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de93880",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1726841129862,
     "user": {
      "displayName": "DAMIANA IOVARO",
      "userId": "12500258692918933283"
     },
     "user_tz": -120
    },
    "id": "1de93880",
    "outputId": "f2250b24-2bf8-4258-f00a-18e6978ddef2"
   },
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8137a917",
   "metadata": {
    "id": "8137a917"
   },
   "source": [
    "# Model Training and Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc956a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "columns = ['bytes_in', 'bytes_out', 'num_pkts_out', 'num_pkts_in', 'proto', 'time_start', 'label']\n",
    "\n",
    "if not GRANULATED:\n",
    "\n",
    "    for time_slice in time_slices:\n",
    "\n",
    "        df = pd.DataFrame(columns=columns)\n",
    "        df.to_csv(f'./LUFlow/GranuleData/clustering_granules_{time_slice}.csv', index=False)\n",
    "\n",
    "        for granule in tqdm(matching_indices_slices[time_slice]):\n",
    "            granule_indeces = matching_indices_slices[time_slice][granule]\n",
    "            granule_data = X_train.loc[granule_indeces]\n",
    "            granule_data_y = y_train.loc[granule_indeces]['label']\n",
    "            \n",
    "            N = len(granule_data)\n",
    "            \n",
    "            if N > 1:\n",
    "                # Vectorized aggregation for sum and start time extraction\n",
    "                row = granule_data[['bytes_in', 'bytes_out', 'num_pkts_out', 'num_pkts_in', 'proto']].sum()\n",
    "                \n",
    "                row['time_start'] = granule_data['time_start'].iloc[0]\n",
    "                \n",
    "                # Vectorized mode calculation\n",
    "                row['label'] = granule_data_y.mode().iloc[0]\n",
    "            else:\n",
    "                row = granule_data.iloc[0]\n",
    "                row['label'] = granule_data_y.iloc[0]\n",
    "\n",
    "            row = pd.DataFrame([row])\n",
    "            row.to_csv(f'./LUFlow/GranuleData/clustering_granules_{time_slice}.csv', mode='a', index=False, header=False)\n",
    "\n",
    "        del granule_data, granule_data_y, row\n",
    "        # gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "J4Gpy_rikamQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2043015,
     "status": "ok",
     "timestamp": 1726843451800,
     "user": {
      "displayName": "DAMIANA IOVARO",
      "userId": "12500258692918933283"
     },
     "user_tz": -120
    },
    "id": "J4Gpy_rikamQ",
    "outputId": "bed7a0eb-ef52-4e0c-bf12-7ea5f2d31c6f"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import math\n",
    "\n",
    "list_reports = {}\n",
    "best_models = {}\n",
    "bcc_reports = {}\n",
    "mcc_reports = {}\n",
    "\n",
    "# Random Forest with GridSearchCV\n",
    "rf_params = {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20]}\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_grid = GridSearchCV(rf_model, rf_params, cv=10, scoring='accuracy')\n",
    "\n",
    "\n",
    "for time_slice in tqdm(time_slices):\n",
    "  granulated_file = f'./LUFlow/GranuleData/clustering_granules_{time_slice}.csv'\n",
    "  granulated_data = pd.read_csv(granulated_file)\n",
    "\n",
    "  x_train = granulated_data.loc[:, granulated_data.columns != 'label']\n",
    "  y_train_list = list(granulated_data['label'])\n",
    "\n",
    "  list_reports[time_slice] = {}\n",
    "  best_models[time_slice] = {}\n",
    "\n",
    "  rf_grid.fit(x_train, y_train_list)\n",
    "  rf_best_model = rf_grid.best_estimator_\n",
    "\n",
    "  rf_pred = rf_best_model.predict(X_test)\n",
    "  report = classification_report(y_test, rf_pred, output_dict=True)\n",
    "\n",
    "  tn, fp, fn, tp = confusion_matrix(y_test, rf_pred).ravel()\n",
    "\n",
    "  bcc_reports[time_slice] = (tp/(tp+fn) + tn/(tn+fp))/2\n",
    "\n",
    "  numerator = (tp*tn - fp*fn)\n",
    "  try:\n",
    "      log_denominator = 0.5 * (np.log(tp + fp) + np.log(tp + fn) + np.log(tn + fp) + np.log(tn + fn))\n",
    "      denominator = np.exp(log_denominator)\n",
    "\n",
    "      # Safely handle the division, in case the denominator is zero\n",
    "      mcc_reports[time_slice] = numerator / denominator if denominator != 0 else float('inf')\n",
    "  except ValueError:\n",
    "      # Handle cases where log is taken on a zero or negative value\n",
    "      mcc_reports[time_slice] = float('nan')\n",
    "\n",
    "\n",
    "  list_reports[time_slice] = pd.DataFrame(report).transpose()\n",
    "  best_models[time_slice] = rf_best_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92521af6",
   "metadata": {
    "id": "92521af6"
   },
   "source": [
    "# Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbeeb4d9",
   "metadata": {},
   "source": [
    "The performance metrics used by the approach proposed in [8]:\n",
    "-\tAccuracy is the proportion of accurately classified datapoints in the test set in relation to the total number of instances.\n",
    "-\tPrecision is the ratio of detected positive cases to all expected positive instances.\n",
    "-\tRecall is the ratio of samples classified as positive to all expected positive instances.\n",
    "-\tF1-score is a harmonic mean of recall and precision.\n",
    "-\tMCC (Matthews Correlation Coefficient) determines the correlation coefficient between the anticipated and actual classifications.\n",
    "-\tBCC (Balanced Accuracy) is used for situations with data imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5e09d192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go up one directory\n",
    "os.chdir('..')\n",
    "\n",
    "# Check if 'reports' directory exists, otherwise create it\n",
    "if not os.path.exists('reports'):\n",
    "    os.mkdir('reports')\n",
    "\n",
    "# Check if 'best_models' directory exists, otherwise create it\n",
    "if not os.path.exists('best_models'):\n",
    "    os.mkdir('best_models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "v000UeVoJMd6",
   "metadata": {
    "id": "v000UeVoJMd6"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import joblib\n",
    "\n",
    "string = 'clustering_'+str(subset)+'_'+time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "\n",
    "os.mkdir(f'./reports/{string}')\n",
    "os.mkdir(f'./best_models/{string}')\n",
    "\n",
    "os.chdir(f'./reports/{string}')\n",
    "\n",
    "for report in list_reports:\n",
    "  list_reports[report].to_csv(f'report_{report}.csv')\n",
    "\n",
    "columns = ['time_slice', 'BCC', 'MCC']\n",
    "df = pd.DataFrame(columns=columns)\n",
    "for time_slice in time_slices:\n",
    "  row = {'time_slice': time_slice, 'BCC': bcc_reports[time_slice], 'MCC': mcc_reports[time_slice]}\n",
    "  row = pd.DataFrame([row])\n",
    "  row.to_csv('bcc_mcc.csv', mode='a', header=False, index=False)\n",
    "\n",
    "os.chdir(f'../../best_models/{string}')\n",
    "\n",
    "for model in best_models:\n",
    "  joblib.dump(best_models[model], f'model_{model}_{string}.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "icbCgYKqIDXX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1726843452604,
     "user": {
      "displayName": "DAMIANA IOVARO",
      "userId": "12500258692918933283"
     },
     "user_tz": -120
    },
    "id": "icbCgYKqIDXX",
    "outputId": "95ece980-dee6-4479-c351-c1348d8b458b"
   },
   "outputs": [],
   "source": [
    "equidistant_x = np.arange(len(time_slices))\n",
    "feature_to_print = 'precision'\n",
    "\n",
    "plt.plot(equidistant_x, [list_reports[i].loc['accuracy'][feature_to_print] for i in time_slices])\n",
    "\n",
    "plt.title(f'{feature_to_print} over time_slices')\n",
    "plt.xticks(equidistant_x, time_slices)\n",
    "plt.xlabel('time slices')\n",
    "plt.ylabel(f'{feature_to_print}')\n",
    "# plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-Jvtsd3wlAOL",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1726843452605,
     "user": {
      "displayName": "DAMIANA IOVARO",
      "userId": "12500258692918933283"
     },
     "user_tz": -120
    },
    "id": "-Jvtsd3wlAOL",
    "outputId": "01f02bfc-fa45-4928-9821-bff3baebe391"
   },
   "outputs": [],
   "source": [
    "for time_slice in time_slices:\n",
    "  print(f'Report {time_slice}: \\n{list_reports[time_slice]}\\n')\n",
    "  print(f'BCC {time_slice}: {bcc_reports[time_slice]}\\n')\n",
    "  print(f'MCC {time_slice}: {mcc_reports[time_slice]}')\n",
    "  print('\\n')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 975848,
     "sourceId": 7295614,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30626,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 459.535705,
   "end_time": "2024-01-02T19:22:34.876445",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-01-02T19:14:55.340740",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
