{"cells":[{"cell_type":"markdown","id":"2caa1f00","metadata":{"id":"2caa1f00"},"source":["# Importing Libraries"]},{"cell_type":"code","execution_count":1,"id":"36751954","metadata":{"id":"36751954"},"outputs":[],"source":["import os\n","import glob\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns"]},{"cell_type":"markdown","id":"zQCuH_8Qx_FK","metadata":{"id":"zQCuH_8Qx_FK"},"source":["# Connection to drive"]},{"cell_type":"code","execution_count":null,"id":"8e541e17","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1727684224260,"user":{"displayName":"DAMIANA IOVARO","userId":"12500258692918933283"},"user_tz":-120},"id":"8e541e17","outputId":"ee9a8e89-2a07-4d2c-a784-9437f929191d"},"outputs":[],"source":["try:\n","    from google.colab import drive\n","    IN_COLAB = True\n","    print(\"Running on Google Colab. \")\n","except:\n","    IN_COLAB = False\n","    print(\"Not running on Google Colab. \")"]},{"cell_type":"code","execution_count":3,"id":"YqDnszP3yBxG","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24352,"status":"ok","timestamp":1727684248607,"user":{"displayName":"DAMIANA IOVARO","userId":"12500258692918933283"},"user_tz":-120},"id":"YqDnszP3yBxG","outputId":"d6dadbf6-66a4-4a8c-f156-1e4675382495"},"outputs":[],"source":["if IN_COLAB:\n","    from google.colab import drive\n","    drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":4,"id":"vKxzLLMyXlXw","metadata":{"id":"vKxzLLMyXlXw"},"outputs":[],"source":["if IN_COLAB:\n","    os.chdir('/content/gdrive/MyDrive/Tesi/dataset')\n","else:\n","    os.chdir('./dataset')"]},{"cell_type":"markdown","id":"0f55febe","metadata":{"id":"0f55febe"},"source":["# Downloading, Reading and Merging Data"]},{"cell_type":"code","execution_count":5,"id":"bf6ace32","metadata":{},"outputs":[],"source":["def move_directories(orig_path, dest_path):\n","    # Check if the destination folder exists, otherwise create it\n","    if not os.path.exists(dest_path):\n","        os.makedirs(dest_path)\n","    \n","    # Iter over all files and folders in the source directory\n","    for item in os.listdir(orig_path):\n","        orig_item_path = os.path.join(orig_path, item)\n","        \n","        # Check if it is a folder\n","        if os.path.isdir(orig_item_path):\n","            # Defines the destination path for the folder\n","            dest_item_path = os.path.join(dest_path, item)\n","            \n","            # Move the folder by renaming it\n","            os.rename(orig_item_path, dest_item_path)\n","            print(f\"Moved directory: {orig_item_path} -> {dest_item_path}\")"]},{"cell_type":"code","execution_count":null,"id":"7f185a12","metadata":{},"outputs":[],"source":["import kagglehub\n","\n","# Check if some subdirectories are not empty\n","path_to_check = \"./LUFlow/\"\n","subdirs = [d for d in os.listdir(path_to_check) if os.path.isdir(os.path.join(path_to_check, d))]\n","non_empty_subdirs = [d for d in subdirs if os.listdir(os.path.join(path_to_check, d))]\n","\n","dest_path = './LUFlow'\n","\n","if non_empty_subdirs:\n","    print(\"Non-empty subdirectories:\", non_empty_subdirs)\n","    print(\"Skip downloading.\")\n","else:\n","    print(\"All subdirectories are empty.\")\n","    print(\"Download dataset.\")\n","\n","    # Download latest version\n","    path = kagglehub.dataset_download(\"mryanm/luflow-network-intrusion-detection-data-set\")\n","\n","    print(\"Path to dataset files:\", path)\n","\n","    move_directories(path, dest_path)"]},{"cell_type":"code","execution_count":null,"id":"2dd63090","metadata":{"id":"2dd63090"},"outputs":[],"source":["df_list = []\n","i = 0\n","\n","for root, dirs, files in os.walk(dest_path):\n","    for file in files:\n","        # checks if 'file' does not exist in the directory\n","        # checks if 'csv' is in the file name\n","        # checks if a particular string is in the file name\n","        # insert in the list only a subset of the existing files\n","        if not os.path.isfile(file) and 'csv' in file and '2021.01' in file and i < 6:\n","            df_list.append(pd.read_csv(os.path.join(root, file)))\n","            i+=1\n","\n","perc = 0.8\n","\n","files_perc = int(len(df_list) * perc)\n","print(f'Num files: {files_perc}')\n","\n","df = pd.concat(df_list[:files_perc])\n","df_test = pd.concat(df_list[files_perc:])\n","\n","df = pd.concat(df_list, ignore_index=True)\n","len(df.columns)"]},{"cell_type":"code","execution_count":null,"id":"957fba95","metadata":{"id":"957fba95"},"outputs":[],"source":["df.info()"]},{"cell_type":"markdown","id":"81d202a8","metadata":{"id":"81d202a8"},"source":["# Exploratory Data Analysis"]},{"cell_type":"code","execution_count":null,"id":"1424708b","metadata":{"id":"1424708b"},"outputs":[],"source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Count the occurrences of each label\n","label_counts = df['label'].value_counts()\n","\n","plt.figure(figsize=(8, 8))\n","plt.pie(label_counts, labels=label_counts.index, autopct=lambda p: '{:.0f}\\n({:.1f}%)'.format(p * sum(label_counts) / 100, p))\n","\n","# Show the plot\n","plt.title('Distribution of Labels')\n","plt.show()"]},{"cell_type":"markdown","id":"e6118ed4","metadata":{"id":"e6118ed4"},"source":["# Data Preprocessing"]},{"cell_type":"code","execution_count":10,"id":"d04422fa","metadata":{"id":"d04422fa"},"outputs":[],"source":["outliers = df['label'] == 'outlier'\n","df = df[~outliers]\n","\n","df = df.reset_index(drop=True)"]},{"cell_type":"code","execution_count":11,"id":"kYo4jc63q_vs","metadata":{"id":"kYo4jc63q_vs"},"outputs":[],"source":["outliers = df_test['label'] == 'outlier'\n","df_test = df_test[~outliers]\n","\n","df_test = df_test.reset_index(drop=True)"]},{"cell_type":"code","execution_count":12,"id":"66c37230","metadata":{"id":"66c37230"},"outputs":[],"source":["columns_to_drop = ['avg_ipt', 'dest_ip', 'dest_port', 'entropy', 'src_ip', 'src_port', 'time_end', 'total_entropy', 'duration']\n","\n","# Drop specified columns\n","df = df.drop(columns=columns_to_drop)\n","\n","df_test = df_test.drop(columns=columns_to_drop)"]},{"cell_type":"code","execution_count":null,"id":"fe1241d8","metadata":{"id":"fe1241d8"},"outputs":[],"source":["df.dropna(axis=1, inplace = True)\n","df_test.dropna(axis=1, inplace = True)\n","\n","missing_values = df.isnull().sum()\n","\n","print(missing_values)"]},{"cell_type":"code","execution_count":14,"id":"78134a11","metadata":{"id":"78134a11"},"outputs":[],"source":["from sklearn.preprocessing import LabelEncoder\n","\n","label_encoder = LabelEncoder()\n","\n","# Fit and transform the label column\n","df['label'] = label_encoder.fit_transform(df['label'])\n","df_test['label'] = label_encoder.fit_transform(df_test['label'])"]},{"cell_type":"code","execution_count":null,"id":"e945052c","metadata":{"id":"e945052c"},"outputs":[],"source":["# Count the occurrences of each label\n","label_counts = df['label'].value_counts()\n","\n","plt.figure(figsize=(8, 8))\n","plt.pie(label_counts, labels=label_counts.index, autopct=lambda p: '{:.0f}\\n({:.1f}%)'.format(p * sum(label_counts) / 100, p))\n","# Show the plot\n","plt.title('Distribution of Labels')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"_tt-YtWchGYq","metadata":{"id":"_tt-YtWchGYq"},"outputs":[],"source":["df.columns"]},{"cell_type":"markdown","id":"k90MxbIk_G_J","metadata":{"id":"k90MxbIk_G_J"},"source":["## Construction of Interval Information Granules\n","\n","### Selecting index using time windows"]},{"cell_type":"code","execution_count":17,"id":"vyXIigQju030","metadata":{"id":"vyXIigQju030"},"outputs":[],"source":["time_slices = [16, 128, 256, 1024, 2048, 3016]\n","\n","matching_indices_slices = {}\n","\n","for ts in time_slices:\n","  # maximum time limit of the granule\n","  limit = df.iloc[0]['time_start'] + ts\n","  # number of groups for time_slice\n","  num_group = 0\n","  matching_indices_slices[ts] = {num_group:[0]}\n","\n","  for i, row in df.iterrows():\n","      # if the value of time_start is greater than limit then limit is updated and\n","      # it is initilized a new group\n","      # otherwise the index of the row is added to matching_indeces_slices\n","      if row['time_start'] >= limit:\n","        limit = row['time_start'] + ts\n","        num_group += 1\n","\n","        matching_indices_slices[ts][num_group] = [i]\n","      else:\n","        # print(ts, num_gran, i)\n","        if i != 0:\n","          matching_indices_slices[ts][num_group].append(i)\n"]},{"cell_type":"markdown","id":"zZMBBfdMA4NV","metadata":{"id":"zZMBBfdMA4NV"},"source":["### Drop Label column"]},{"cell_type":"code","execution_count":18,"id":"d96cee97","metadata":{"id":"d96cee97"},"outputs":[],"source":["# Separate features (X) and target variable (y)\n","X_train = df.drop('label', axis=1)\n","y_train = df['label']\n","\n","X_test = df_test.drop('label', axis=1)\n","y_test = df_test['label']"]},{"cell_type":"code","execution_count":null,"id":"911c3e12","metadata":{"id":"911c3e12"},"outputs":[],"source":["X_train"]},{"cell_type":"code","execution_count":null,"id":"1de93880","metadata":{"id":"1de93880"},"outputs":[],"source":["y_train"]},{"cell_type":"code","execution_count":21,"id":"2058ae5b","metadata":{"id":"2058ae5b"},"outputs":[],"source":["from sklearn.model_selection import GridSearchCV\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import classification_report\n","from tqdm import tqdm\n","import math"]},{"cell_type":"markdown","id":"t8o0K9k6npXH","metadata":{"id":"t8o0K9k6npXH"},"source":["### Creating interval information granules\n","\n","with justifiable granularity principle\n"]},{"cell_type":"markdown","id":"cbc1eca3","metadata":{},"source":["implementing both 'a' and 'b' to define the frames to include in the granule"]},{"cell_type":"code","execution_count":null,"id":"_dvzK55YBC1D","metadata":{"id":"_dvzK55YBC1D"},"outputs":[],"source":["X_train_granule = {}\n","y_train_granule = {}\n","\n","experimental_data = {}\n","\n","alpha = 2.0\n","l = 100\n","\n","for time_slice in tqdm(time_slices):\n","  X_train_granule[time_slice] = {}\n","  y_train_granule[time_slice] = {}\n","\n","  experimental_data[time_slice] = {}\n","\n","  for group in matching_indices_slices[time_slice]:\n","\n","    V_b_opt = float('-inf')\n","    V_a_opt = float('-inf')\n","    # obtains index for previous defined groups\n","    group_indeces = matching_indices_slices[time_slice][group]\n","\n","    experimental_data[time_slice][group] = X_train.iloc[group_indeces]\n","\n","    N = len(experimental_data[time_slice][group])\n","\n","    # if the slice has only one element, the granule is the element itself\n","    if N == 1:\n","      X_train_granule[time_slice][group] = experimental_data[time_slice][group]\n","      y_train_granule[time_slice][group] = y_train.iloc[X_train_granule[time_slice][group].index]\n","      continue\n","\n","    m = experimental_data[time_slice][group]['time_start'].mean()\n","    y_max = experimental_data[time_slice][group]['time_start'].max()\n","\n","    delta_y = (y_max - m)/l\n","\n","    for h in range(0, l):\n","      b = m+h*delta_y\n","      a = m-h*delta_y\n","\n","      condition = (experimental_data[time_slice][group]['time_start'] > m) & (experimental_data[time_slice][group]['time_start'] <= b)\n","      cov_b = len(experimental_data[time_slice][group][condition])/N\n","      sp_b = math.exp(-alpha*abs(m-b))\n","\n","      condition = (experimental_data[time_slice][group]['time_start'] > a) & (experimental_data[time_slice][group]['time_start'] <= m)\n","      cov_a = len(experimental_data[time_slice][group][condition])/N\n","      sp_a = math.exp(-alpha*abs(a-m))\n","\n","      V_b = cov_b*sp_b\n","      V_a = cov_a*sp_a\n","\n","      if V_a > V_a_opt:\n","        a_opt = a\n","        V_a_opt = V_a\n","\n","      if V_b > V_b_opt:\n","        b_opt = b\n","        V_b_opt = V_b\n","\n","    # Make sure the granule is not empty\n","    # filtered_granule = experimental_data[time_slice][group][(experimental_data[time_slice][group] > a_opt) & \n","    #                                                       (experimental_data[time_slice][group] <= b_opt)]\n","\n","    condition = (experimental_data[time_slice][group]['time_start'] > a_opt) & (experimental_data[time_slice][group]['time_start'] <= b_opt)\n","    filtered_granule = experimental_data[time_slice][group][condition]\n","\n","    if filtered_granule.empty:\n","      continue  # Skip this group if no data fits the condition\n","\n","    X_train_granule[time_slice][group] = filtered_granule\n","    # Extract the selected rows\n","    selected_rows = y_train.iloc[X_train_granule[time_slice][group].index]\n","\n","    # Find the most frequent value (mode)\n","    most_frequent_value = selected_rows.mode().iloc[0]\n","\n","    # Assign the modal value to all selected rows\n","    y_train.iloc[X_train_granule[time_slice][group].index] = most_frequent_value\n","    y_train_granule[time_slice][group] = y_train.iloc[X_train_granule[time_slice][group].index]"]},{"cell_type":"markdown","id":"rMYodOgOo0nB","metadata":{"id":"rMYodOgOo0nB"},"source":["### Training of Random Forest models"]},{"cell_type":"code","execution_count":null,"id":"J4Gpy_rikamQ","metadata":{"id":"J4Gpy_rikamQ"},"outputs":[],"source":["from tqdm import tqdm\n","\n","list_reports = {}\n","best_models = {}\n","\n","# Random Forest with GridSearchCV\n","rf_params = {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20]}\n","rf_model = RandomForestClassifier()\n","rf_grid = GridSearchCV(rf_model, rf_params, cv=10, scoring='accuracy')\n","\n","\n","for time_slice in tqdm(time_slices):\n","  y_train_list = pd.concat(y_train_granule[time_slice].values(), ignore_index=True)\n","  #y_train_list = list(y_train_granule[time_slice].values())\n","  X_train_granule_df = pd.concat(X_train_granule[time_slice].values(), ignore_index=True)\n","\n","  list_reports[time_slice] = {}\n","  best_models[time_slice] = {}\n","\n","  rf_grid.fit(X_train_granule_df, y_train_list)\n","  rf_best_model = rf_grid.best_estimator_\n","\n","  rf_pred = rf_best_model.predict(X_test)\n","  report = classification_report(y_test, rf_pred, output_dict=True)\n","\n","  list_reports[time_slice] = pd.DataFrame(report).transpose()\n","  best_models[time_slice] = rf_best_model"]},{"cell_type":"markdown","id":"92521af6","metadata":{"id":"92521af6"},"source":["# Model Performance"]},{"cell_type":"code","execution_count":24,"id":"8f874b93","metadata":{},"outputs":[],"source":["# Go up one directory\n","os.chdir('..')\n","\n","# Check if 'reports' directory exists, otherwise create it\n","if not os.path.exists('reports'):\n","    os.mkdir('reports')\n","\n","# Check if 'best_models' directory exists, otherwise create it\n","if not os.path.exists('best_models'):\n","    os.mkdir('best_models')"]},{"cell_type":"code","execution_count":25,"id":"v000UeVoJMd6","metadata":{"id":"v000UeVoJMd6"},"outputs":[],"source":["import time\n","import joblib\n","\n","string = time.strftime(\"%Y%m%d-%H%M%S\")\n","#string = 'justifiablegranularity_complete'         \n","\n","os.mkdir(f'./reports/{string}')\n","os.mkdir(f'./best_models/{string}')\n","\n","os.chdir(f'./reports/{string}')\n","\n","for report in list_reports:\n","  list_reports[report].to_csv(f'report_{report}.csv')\n","\n","\n","os.chdir(f'../../best_models/{string}')\n","\n","for model in best_models:\n","  joblib.dump(best_models[model], f'model_{model}_{string}.joblib')"]},{"cell_type":"code","execution_count":null,"id":"icbCgYKqIDXX","metadata":{"id":"icbCgYKqIDXX"},"outputs":[],"source":["equidistant_x = np.arange(len(time_slices))\n","feature_to_print = 'precision'\n","\n","plt.plot(equidistant_x, [list_reports[i].loc['accuracy'][feature_to_print] for i in time_slices])\n","\n","plt.title(f'{feature_to_print} over time_slices')\n","plt.xticks(equidistant_x, time_slices)\n","plt.xlabel('time slices')\n","plt.ylabel(f'{feature_to_print}')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"-Jvtsd3wlAOL","metadata":{"id":"-Jvtsd3wlAOL"},"outputs":[],"source":["for time_slice in time_slices:\n","  print(f'Report {time_slice}: \\n{list_reports[time_slice]}')\n","  print('\\n')"]}],"metadata":{"colab":{"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"datasetId":975848,"sourceId":7295614,"sourceType":"datasetVersion"}],"dockerImageVersionId":30626,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"tesi","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.5"},"papermill":{"default_parameters":{},"duration":459.535705,"end_time":"2024-01-02T19:22:34.876445","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-01-02T19:14:55.340740","version":"2.4.0"}},"nbformat":4,"nbformat_minor":5}
